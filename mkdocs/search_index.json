{
    "docs": [
        {
            "location": "/", 
            "text": "BOSCO\n\n\nIntroduction\n\n\nBOSCO is a job submission manager designed to help researchers manage\nlarge numbers (~1000s) of job submissions to the different resources\nthat they can access on a campus (initially a PBS cluster running\nLinux).\n\n\nIf you find any problems or need\nhelp installing or running BoSCO, please email\n\nbosco-discuss@opensciencegrid.org\n.\n\n\nBosco offers the following capabilities:\n\n\n\n\nJobs are automatically resubmitted when they fail. The researcher\n    does not need to babysit their jobs.\n\n\nJob submissions can be throttled to meet batch scheduler\n    settings (e.g. only 10 jobs running concurrently). The researcher\n    does not need to make multiple submissions. BOSCO handles that\n    for them.\n\n\nBOSCO is designed to be flexible and allows jobs to be submitted to\n    multiple clusters, with different job schedulers (e.g. PBS, LSF,\n    HTCondor, SGE, and SLURM).\n\n\n\n\nThe primary advantage for the researcher is that they only need to learn\none job scheduler environment even if the clusters utilize different\nnative environments.\n\n\nBOSCO definitions, versions and documents\n\n\n\n\n\n\n\nThe BOSCO submit node is the host where BOSCO is installed and where\nuser's login to submit jobs via BOSCO. \n\n\nThe multiple clusters added to\nBOSCO (i.e. where the user's can submit jobs via BOSCO) are referred as\nBOSCO resources. \n\n\nBOSCO resources can be a traditional cluster or a pool\nof resources connected via HTCondor, another BOSCO installation, Grid,\nor some other technology. Anyway all BOSCO resources have a submit node\n(where you would login to submit jobs if you had no BOSCO) and worker\nnodes (where the jobs run).\n\n\n\n\nThis page explains how to use BOSCO. Before using BOSCO you, or someone\nfor you, will have to install a version of BOSCO and add at least one\nBOSCO resource. Adding, testing and removing BOSCO resources is part of\nthe BOSCO configuration.\n\n\nBOSCO Single User\n allows a researcher to install BOSCO\nin her/his (non-privileged) account, to configure it and to use it. If\nyou plan (at least initially) to connect a single cluster (BOSCO Single\nUser Single Cluster) then use the \nBOSCO Quick Start\nGuide\n. Otherwise, to install and configure BOSCO\nSingle-user read \nBOSCO Single User Installation\n.\n\n\nBOSCO Multi User\n is installed, configured and started\non a host by the system administrator (root) and is available to all the\nusers on the host. To install and configure BOSCO Multi-user read \nBOSCO\nMulti User Installation\n.\n\n\nLater in this document we'll assume that BOSCO has been already\ninstalled and configured correctly. For the installation or to change\nthe configuration (e.g. to add or remove BOSCO resources) please check\nthe other documents: \nBOSCO Single User Installation\n and\n\nBOSCO Multi User Installation\n.\n\n\nRequirements\n\n\nThere are specific requirements for the BOSCO resources that are\nspecified in the install documents.\n\n\nTo use BOSCO you need a BOSCO submit host with BOSCO installed and\nconfigured correctly. All requirements for the BOSCO submit host and the\nBOSCO resources, as well as the requirements to include BOSCO in a more\ncomplex Condor setup, are described in the install documents.\n\n\nHow to Install\n\n\nEither you or a system administrator for you will have to install and\nsetup BOSCO. The installation consists in downloading and installing the\nBOSCO software. The setup consists in managing which clusters are\nincluded in the BOSCO pool and will execute your jobs; it includes all\nthe operations performed using the \nbosco_cluster\n command. The\ninstallation and setup are covered in two separate documents:\n\n\n\n\nTo install and setup BOSCO so that it is used and configured from a\n    single user account please refer to \nBosco Installation\n\n\nTo install and setup BOSCO so that it is configured from a single\n    user account but it can be used by all the accounts on the host\n    please refer to \nBosco Multiuser\n\n\n\n\nHow to Use\n\n\nIn order to use BOSCO and submit a job:\n\n\n\n\nBOSCO must be installed\n\n\nBOSCO must be running (it must have been started)\n\n\nAt least one cluster must have been added to BOSCO\n\n\nYou must setup the environment\n\n\n\n\nStarting/Stopping and Configuring BOSCO\n\n\nEach time I mention \"you\" in this section refers either to you or to a system administrator\nthat acts on your behalf, probably the same person that installed BOSCO.\n\n\nBOSCO has some persistent services that must be running. You'll have to\nstart it at the beginning and probably after each reboot of your host.\nYou should stop BOSCO before an upgrade and possibly before a shutdown\nof your host. If you will not use BOSCO anymore, uninstall will remove\nit from your system.\n\n\nYou need to add to BOSCO all the clusters of which you like to use the\nresources. In order to run jobs you need at least one.\n\n\nPlease refer to the \nBosco Installation\n or \nBosco Multiuser\n documents for\noperations including:\n\n\n\n\nstarting BOSCO\n\n\nstopping BOSCO\n\n\nupdating BOSCO\n\n\nuninstalling BOSCO\n\n\nadding one or more clusters to BOSCO\n\n\n\n\nSetup\n\n\nSetup the environment before using Since BOSCO is not installed in\nthe system path. An environment file must be sourced all the times you\nuse BOSCO (start/stop/job submission or query, anything):\n\n\nsource ~/bosco/bosco_setenv\n\n\n\n\n\n\n\nTroubleshooting\n\n\nUseful Configuration and Log Files\n\n\nBOSCO underneath is using\nCondor. You can find all the Condor log files in\n\n~/bosco/local.HOSTNAME/log\n\n\nKnown Issues\n\n\n\n\nMake sure that BOSCO is running. BOSCO may not survive after you\nlog out. When you log back in after sourcing the setup\n(\nsource ~/bosco/bosco_setenv\n), if you are using BOSCO single-user you\nshould start BOSCO (\nbosco_start\n) specially if the command \ncondor_q\n\nis failing. More details about starting BOSCO are in BoscoInstall and\nBoscoMultiUser\n\n\n\n\nGet Help/Support\n\n\nTo get assistance you can send an email to", 
            "title": "Introduction"
        }, 
        {
            "location": "/#bosco", 
            "text": "", 
            "title": "BOSCO"
        }, 
        {
            "location": "/#introduction", 
            "text": "BOSCO is a job submission manager designed to help researchers manage\nlarge numbers (~1000s) of job submissions to the different resources\nthat they can access on a campus (initially a PBS cluster running\nLinux).  If you find any problems or need\nhelp installing or running BoSCO, please email bosco-discuss@opensciencegrid.org .  Bosco offers the following capabilities:   Jobs are automatically resubmitted when they fail. The researcher\n    does not need to babysit their jobs.  Job submissions can be throttled to meet batch scheduler\n    settings (e.g. only 10 jobs running concurrently). The researcher\n    does not need to make multiple submissions. BOSCO handles that\n    for them.  BOSCO is designed to be flexible and allows jobs to be submitted to\n    multiple clusters, with different job schedulers (e.g. PBS, LSF,\n    HTCondor, SGE, and SLURM).   The primary advantage for the researcher is that they only need to learn\none job scheduler environment even if the clusters utilize different\nnative environments.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#bosco-definitions-versions-and-documents", 
            "text": "The BOSCO submit node is the host where BOSCO is installed and where\nuser's login to submit jobs via BOSCO.   The multiple clusters added to\nBOSCO (i.e. where the user's can submit jobs via BOSCO) are referred as\nBOSCO resources.   BOSCO resources can be a traditional cluster or a pool\nof resources connected via HTCondor, another BOSCO installation, Grid,\nor some other technology. Anyway all BOSCO resources have a submit node\n(where you would login to submit jobs if you had no BOSCO) and worker\nnodes (where the jobs run).   This page explains how to use BOSCO. Before using BOSCO you, or someone\nfor you, will have to install a version of BOSCO and add at least one\nBOSCO resource. Adding, testing and removing BOSCO resources is part of\nthe BOSCO configuration.  BOSCO Single User  allows a researcher to install BOSCO\nin her/his (non-privileged) account, to configure it and to use it. If\nyou plan (at least initially) to connect a single cluster (BOSCO Single\nUser Single Cluster) then use the  BOSCO Quick Start\nGuide . Otherwise, to install and configure BOSCO\nSingle-user read  BOSCO Single User Installation .  BOSCO Multi User  is installed, configured and started\non a host by the system administrator (root) and is available to all the\nusers on the host. To install and configure BOSCO Multi-user read  BOSCO\nMulti User Installation .  Later in this document we'll assume that BOSCO has been already\ninstalled and configured correctly. For the installation or to change\nthe configuration (e.g. to add or remove BOSCO resources) please check\nthe other documents:  BOSCO Single User Installation  and BOSCO Multi User Installation .", 
            "title": "BOSCO definitions, versions and documents"
        }, 
        {
            "location": "/#requirements", 
            "text": "There are specific requirements for the BOSCO resources that are\nspecified in the install documents.  To use BOSCO you need a BOSCO submit host with BOSCO installed and\nconfigured correctly. All requirements for the BOSCO submit host and the\nBOSCO resources, as well as the requirements to include BOSCO in a more\ncomplex Condor setup, are described in the install documents.", 
            "title": "Requirements"
        }, 
        {
            "location": "/#how-to-install", 
            "text": "Either you or a system administrator for you will have to install and\nsetup BOSCO. The installation consists in downloading and installing the\nBOSCO software. The setup consists in managing which clusters are\nincluded in the BOSCO pool and will execute your jobs; it includes all\nthe operations performed using the  bosco_cluster  command. The\ninstallation and setup are covered in two separate documents:   To install and setup BOSCO so that it is used and configured from a\n    single user account please refer to  Bosco Installation  To install and setup BOSCO so that it is configured from a single\n    user account but it can be used by all the accounts on the host\n    please refer to  Bosco Multiuser", 
            "title": "How to Install"
        }, 
        {
            "location": "/#how-to-use", 
            "text": "In order to use BOSCO and submit a job:   BOSCO must be installed  BOSCO must be running (it must have been started)  At least one cluster must have been added to BOSCO  You must setup the environment", 
            "title": "How to Use"
        }, 
        {
            "location": "/#startingstopping-and-configuring-bosco", 
            "text": "Each time I mention \"you\" in this section refers either to you or to a system administrator\nthat acts on your behalf, probably the same person that installed BOSCO.  BOSCO has some persistent services that must be running. You'll have to\nstart it at the beginning and probably after each reboot of your host.\nYou should stop BOSCO before an upgrade and possibly before a shutdown\nof your host. If you will not use BOSCO anymore, uninstall will remove\nit from your system.  You need to add to BOSCO all the clusters of which you like to use the\nresources. In order to run jobs you need at least one.  Please refer to the  Bosco Installation  or  Bosco Multiuser  documents for\noperations including:   starting BOSCO  stopping BOSCO  updating BOSCO  uninstalling BOSCO  adding one or more clusters to BOSCO", 
            "title": "Starting/Stopping and Configuring BOSCO"
        }, 
        {
            "location": "/#setup", 
            "text": "Setup the environment before using Since BOSCO is not installed in\nthe system path. An environment file must be sourced all the times you\nuse BOSCO (start/stop/job submission or query, anything):  source ~/bosco/bosco_setenv", 
            "title": "Setup"
        }, 
        {
            "location": "/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/#useful-configuration-and-log-files", 
            "text": "BOSCO underneath is using\nCondor. You can find all the Condor log files in ~/bosco/local.HOSTNAME/log", 
            "title": "Useful Configuration and Log Files"
        }, 
        {
            "location": "/#known-issues", 
            "text": "Make sure that BOSCO is running. BOSCO may not survive after you\nlog out. When you log back in after sourcing the setup\n( source ~/bosco/bosco_setenv ), if you are using BOSCO single-user you\nshould start BOSCO ( bosco_start ) specially if the command  condor_q \nis failing. More details about starting BOSCO are in BoscoInstall and\nBoscoMultiUser", 
            "title": "Known Issues"
        }, 
        {
            "location": "/#get-helpsupport", 
            "text": "To get assistance you can send an email to", 
            "title": "Get Help/Support"
        }, 
        {
            "location": "/BoscoInstall/", 
            "text": "Bosco Installation\n\n\nIntroduction\n\n\nBOSCO is a job\nsubmission manager designed to help researchers manage large numbers\n(\\~1000s) of job submissions to the different resources that they can\naccess on a campus. This is release 1.2 of BOSCO, if you find any\nproblems or need help installing or running Bosco, please email \n.\n\n\nIt offers the following capabilities:\n\n\n\n\nJobs are automatically resubmitted when they fail. The researcher\n    does not need to babysit their jobs.\n\n\nJob submissions can be throttled to meet batch scheduler\n    settings (e.g. only 10 jobs running concurrently). The researcher\n    does not need to make multiple submissions. BOSCO handles that\n    for them.\n\n\nBOSCO is designed to be flexible and allows jobs to be submitted to\n    multiple clusters, with different job schedulers (e.g. PBS, LSF,\n    SGE, HTCondor, SLURM*).\n\n\n\n\nThe primary advantage for the researcher is that they only need to learn\none job scheduler environment even if the clusters utilize different\nnative environments.\n\n\n* SLURM support is via its PBS emulation\n\n\nThis document explains how to install, configure and use BOSCO for a\nsingle user. We recommend to use the \nBosco Quick Start\n guide (less\nflexible but easier and guided setup), if you plan to install BOSCO only\nfor you (single user) and to connect it to only one cluster.\n\nBosco Quick Start\n will give you a full installation but to learn how to\nconnect to multiple resources you have to read this document, it is not\nexplained in the quick start guide.\n\n\nRequirements\n\n\n\n\n\n\nBOSCO Submit-node\n\n:   This is the system that the researcher uses to submit jobs. In\n    general it can be the user's laptop, workstation, or it can be\n    another system that the user logs into for submitting jobs to\n    the cluster. \nThere can not be any Condor collector running on the\n    submit node\n, otherwise it will conflict with Bosco.\n\n\n\n\n\n\nBOSCO resource (aka Cluster) submit-node\n\n:   This is the node that you normally login to on the PBS, SGE, LSF,\n    SLURM or HTCondor cluster (the BOSCO resource that you'd like\n    to add).\n\n\n\n\n\n\nPBS flavors supported\n\n:   Torque and PBSPro\n\n\n\n\n\n\nHTCondor flavors supported\n\n:   HTCondor 7.6 or later\n\n\n\n\n\n\nSGE flavors supported\n\n:   no special requirements (Sun Grid Engine and other Grid Engine\n    versions supported)\n\n\n\n\n\n\nLSF flavors\n\n:   no special requirements\n\n\n\n\n\n\nSLURM flavors\n\n:   no special requirements\n\n\n\n\n\n\nBOSCO resource (aka Cluster)\n\n:   This is the remote cluster that jobs will execute on (the\n    BOSCO resource). The Cluster submit-node is a node belonging to\n    this cluster. The nodes where jobs run are referred as worker nodes.\n    All the cluster needs:\n\n\n\n\n\n\nShared Filesystem\n\n    :   The Cluster needs a shared home filesystem (if the Cluster has         no shared filesystem only Grid universe jobs can be sent to it)\n\n\n\n\n\n\nNetwork Access\n\n    :   The worker nodes need to have access to the submit host. The         worker nodes can be behind a         \nNAT\n         between the worker nodes and the submit host.\n\n\n\n\n\n\n\n\nNote\n\n\nThe BOSCO resource requirements\njust listed (shared file system and outbound network access) are\nrequired only if you submit jobs using the HTCondor \nvanilla\n universe.\nIf you submit jobs using the \ngrid universe\n, submitting to one\nresource at the time, then both requirements can be relaxed. I.e. there\nis \nno need of a shared file system or access to the BOSCO submit host\nfrom the BOSCO resource\n.\n\n\n\n\nBOSCO can be used as part of a more complex Condor setup (with flocking\nor multiple pools). Whatever the setup:\n\n\n\n\nthe BOSCO host needs connectivity to the cluster submit nodes of the\n    BOSCO resources\n\n\nthe worker nodes of the BOSCO resources (running the jobs, e.g. the\n    nodes in the PBS cluster) must have network connectivity to the jobs\n    submit node (the BOSCO host or a different Condor schedd flocking\n    into it)\n\n\n\n\nNetworking\n\n\nBOSCO submit host\n\n\nThe BOSCO submit host requires no ports open for \nuniverse = grid\n jobs.  For \n\nuniverse = vanilla\n jobs, port 11000 is required.\n\n\n\n\nNote\n\n\n\n\nThe port 11000 is required\n\nONLY\n if \"vanilla\" jobs are submitted. \"vanilla\" jobs require also\nthat all worker nodes be able to reach the submit host: this means that\nthe submit host has to have a public IP address or at least an address\nknown by all BOSCO resources that are going to be added. If you need to\nuse a port different from 11000, you can edit the BOSCO configuration.\n\n\nBOSCO Resources\n\n\nThe cluster login node requires port 22 from the BOSCO submit host.  BOSCO will\nuse the SSH protocol to communicate with the login node.\n\n\nThe second requirement is for the BOSCO worker nodes (wherever the jobs\nwill run) and it is needed \nonly if \"vanilla\" jobs are used\n.  The worker nodes must\nbe able to connect to the BOSCO submit host.  Only outgoing from the worker node to the \nBOSCO submit host is required.\n\n\nHow to Install\n\n\nInstallation Procedure\n\n\n\n\nDownload\n Bosco from\n    the bosco website. We recommend the \nMulti-Platform\n installer. It\n    is a bit more complex than the \nquick start\n    installation\n but allows more options.\n\n\nUntar Bosco from the command line: \npre class=\"screen\"\n\n\n\n\n$ cd Downloads \n\n  $ tar xzf boscoinstaller.tar.gz\n\n\n\n\nRun the installer:\n\n\n\n\n$ python boscoinstaller\n\n\nHow to Use\n\n\nNow BOSCO is installed. To use it:\n   1. Setup the environment\n   1. Add all the desired clusters (at least one)\n   1. Start BOSCO\n   1. Submit a test job\n   1. Ready to submit a real job\n\n\nSetup environment before using\n\n\nSince BOSCO is not installed in the system path. An environment file must be sourced all the times you use BOSCO (start/stop/job submission or query, anything):\n\n\n$ source ~/bosco/bosco_setenv\n\n\nStarting BOSCO\n\n\nBOSCO has some persistent services that must be running. You'll have to start it at the beginning and probably after each reboot of your host.\nYou should stop BOSCO before an upgrade and possibly before a shutdown of your host.\nIf you will not use BOSCO anymore, \nbosco_uninstall\n will remove it from your system.\n\n\nTo start BOSCO:\n\n\n$ bosco_start\n\n\nAdd a cluster to BOSCO\n\n\nTo add a new cluster to the resources you will be using through BOSCO:\n\n\n\n\nSetup the environment appropriate for your shell as described in the \nsetup environment section\n (above).\n\n\nFor the cluster \nmycluster\n with user \nusername\n and submit host \nmycluster-submit.mydomain.org\n (Fully Qualified Domain Name, aka full hostname including the domain name) and Local Resource Management System \nLRMS\n (valid values are =pbs=, =condor=, =sge= or =lsf=). Replace the parts in red: \n\n%UCL_PROMPT% bosco_cluster --add %RED%username@mycluster-submit.mydomain.org LRMS%ENDCOLOR% \n\n %TWISTY{%TWISTY_OPTS_DETAILED% }%   \n\n-bash-3.2$ bosco_cluster -add itbv-ce-pbs.uchicago.edu\nEnter password to copy ssh keys to itbv-ce-pbs.uchicago.edu:\nThe authenticity of host 'itbv-ce-pbs.uchicago.edu (128.135.158.176)' can't be established.\nRSA key fingerprint is 8e:a6:db:18:80:6b:b7:de:56:c8:5a:a2:75:19:11:8d.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'itbv-ce-pbs.uchicago.edu,128.135.158.176' (RSA) to the list of known hosts.\nInstalling BOSCO on itbv-ce-pbs.uchicago.edu...\nInstallation complete\n\n %ENDTWISTY%\n\n\n\n\nWhen you add your first cluster, BOSCO will prompt you for a password that will be used to store the SSH keys used by BOSCO to access all your clusters (=Enter password for bosco ssh key:=). Select a random string. It is preferable if you do not use the password you use to access the cluster or to unlock your SSH keys.\n\n\nThen, if you don't have a ssh key agent with that cluster enabled, you will be prompted for the password that you use to access the cluster you are adding to BOSCO (=Enter password to copy ssh keys to ...=). This may be followed by a confirmation of the RSA key fingerprint, if it is your first ssh connection from this host, where you have to answer =yes=.\n\n\n%IMPORTANT% You must be able to login to the remote cluster. If password authentication is OK, the script will ask you for your password. If key only login is allowed, then you must load your key in the =ssh-agent=. Here is an example adding the key and testing the login: %TWISTY{%TWISTY_OPTS_DETAILED% }%   \n\n%UCL_PROMPT% eval \nssh-agent\n\nAgent pid 17103;\n%UCL_PROMPT% ssh-add id_rsa_bosco \nEnter passphrase for id_rsa_bosco: \nIdentity added: id_rsa_bosco (id_rsa_bosco)\n%UCL_PROMPT% ssh  uc3@itbv-ce-pbs\nLast login: Thu Sep 13 13:49:33 2012 from uc3-bosco.mwt2.org\n$ logout\n\n\n%ENDTWISTY%\n\n\n%IMPORTANT% Some clusters have multiple login nodes behind a round robin DNS server. You can recognize them because when you login to the node (e,g: ==ssh login.mydomain.org==), it will show a name different form the one used to connect (e.g.: ==hostname -f== will return ==login2.mydomain.org==). If this happens you must add the BOSCO resources by using a name of the host, not the DNS alias (e.g. ==bosco_cluster --add login2.mydomain.org==). This is because sometime these multiple login nodes do not share all the directories and BOSCO may be unable to find its files if different connections land on different hosts : %TWISTY{%TWISTY_OPTS_DETAILED% }%  Note how =midway-login2.rcc.uchicago.edu= is use instead of =midway.rcc.uchicago.edu=: \n\n%UCL_PROMPT% ssh  mm@%RED%midway.rcc.uchicago.edu%ENDCOLOR%\nmm@midway.rcc.uchicago.edu's password: \n===============================================================================\n                               Welcome to Midway\n                           Research Computing Center\n                             University of Chicago\n...\n===============================================================================\n\n\n[mm@midway-login2 ~]$ hostname -f\n%RED%midway-login2.rcc.uchicago.edu%ENDCOLOR%\n[mm@midway-login2 ~]$ logout\nConnection to midway.rcc.uchicago.edu closed.\n%UCL_PROMPT% marco$ bosco_cluster --add mm@%RED%midway-login2.rcc.uchicago.edu%ENDCOLOR%\nWarning: No batch system specified, defaulting to PBS\nIf this is incorrect, rerun the command with the batch system specified\n\n\nEnter the password to copy the ssh keys to mm@midway-login2.rcc.uchicago.edu:\nmm@midway-login2.rcc.uchicago.edu's password: \nDetecting PBS cluster configuration...bash: cannot set terminal process group (-1): Invalid argument\nbash: no job control in this shell\nbash: qmgr: command not found\nDone!\nDownloading for mm@midway-login2.rcc.uchicago.edu....\nUnpacking.........\nSending libraries to mm@midway-login2.rcc.uchicago.edu...\nCreating BOSCO for the WN's....................................................................\nInstallation complete\nThe cluster mm@midway-login2.rcc.uchicago.edu has been added to BOSCO\nIt is available to run jobs submitted with the following values:\n\n\n\n\nuniverse = grid\ngrid_resource = batch pbs mm@midway-login2.rcc.uchicago.edu\n%UCL_PROMPT%\n\n\n%ENDTWISTY%\n\n\n\n\nWhen adding the cluster, if the last message is =Done!=. Your cluster has been added successfully.\n\n\nYou can see a list of the current clusters in BOSCO by typing:\n\n%UCL_PROMPT% bosco_cluster --list\n\n\n%TWISTY{%TWISTY_OPTS_DETAILED% }%   \n\n-bash-3.2$ bosco_cluster --list\nitbv-ce-pbs.uchicago.edu\n\n\n%ENDTWISTY%\n%ENDSECTION{\"BoscoAddResource\"}%\n\n\n---## Submitting a test job\nYou can send a simple test job to verify that the cluster added is working correctly.\n\n\nTo send a BOSCO test job to the host %RED%username@mycluster-submit.mydomain%ENDCOLOR% (name as listed in the output of =bosco_cluster --list=):\n   1. Setup the environment appropriate for your shell as described in the setup environment section (above).\n   1. For the cluster %RED%username@mycluster-submit.mydomain%ENDCOLOR% (identical to output of  =bosco_cluster --list=). Replace the parts in red: \n\n%UCL_PROMPT% bosco_cluster --test %RED%username@mycluster-submit.mydomain%ENDCOLOR%\n\n %TWISTY{%TWISTY_OPTS_DETAILED% }%   \n\n%UCL_PROMPT% $ bosco_cluster -t dweitzel@ff-grid.unl.edu\ndweitzel@ff-grid.unl.edu\nTesting ssh to dweitzel@ff-grid.unl.edu...Passed!\nTesting bosco submission...Passed!\nChecking for submission to remote pbs cluster (could take ~30 seconds)...Passed!\nSubmission files for these jobs are in /home/dweitzel/bosco/local.localhocentos56/bosco-test\nExecution on the remote cluster could take a while...Exiting\n\n %ENDTWISTY%\n%ENDSECTION{\"BoscoUseShort\"}%\n\n\n---## How to Stop and Remove\nTo stop BOSCO:\n\n%UCL_PROMPT% bosco_stop\n\n\n\nTo uninstall BOSCO:\n   * If you want to remove remote clusters get the list and remove them one by one: \n%UCL_PROMPT% bosco_cluster --list\n\n\nFor each remote cluster\n\n\n%UCL_PROMPT% bosco_cluster -r %RED%user@cluster_as_spelled_in_list%ENDCOLOR%\n\n   * Remove the installation directory:  \n%UCL_PROMPT% bosco_uninstall\n\n\n%NOTE% Uninstalling BOSCO removes the software but leaves the files in your =.bosco= and =.ssh= directories with all the information about the added clusters and the SSH keys. Files installed on the remote clusters are not removed either. \n\n\n---## How to Update BOSCO\nIf you want to update BOSCO to a new version you have to:\n   1. setup BOSCO:\n%UCL_PROMPT% source ~/bosco/bosco_setenv\n\n   1. stop BOSCO: \n%UCL_PROMPT% bosco_stop\n\n   1. remove the old BOSCO: \n%UCL_PROMPT% bosco_uninstall\n\n   1. download and install the new BOSCO (see install section above)  and re-add all the clusters in your setup:\n   1. for each installed cluster (the list is returned by =bosco_cluster --list=):\n      1. remove the cluster: \n%UCL_PROMPT% bosco_cluster --remove %RED%username@mycluster-submit.mydomain.org%ENDCOLOR% \n\n      1. add the cluster: \n%UCL_PROMPT% bosco_cluster --add %RED%username@mycluster-submit.mydomain.org queue%ENDCOLOR% \n\n   1. start BOSCO: \n%UCL_PROMPT% bosco_start\n\n\nThis will update the local installation and the software on the remote clusters\n%ENDSECTION{\"BoscoSetup\"}%\n\n\n%STARTSECTION{\"BoscoJob\"}%\n---# Job submission example\n\n\nYou can submit a regular Condor vanilla job to BOSCO. The Campus Factory, a component within BOSCO, will take care to run it on the different clusters that you added and to transfer the input and output files as needed.\nHere is a simple example. The Condor team provides [[http://research.cs.wisc.edu/condor/tutorials/][many great tutorials]] to learn more.\n\n\n---## Configuring Executable\nYour may wrap your job in a script (e.g. using your favorite shell or Python) so that you can invoke more executables and do some tests.\n\n\nThis is a simple bash script, called =myjob.sh=: \n\n\n!/bin/bash\n\n\nPrepare for the execution\n\n\nRun the actual applications\n\n\nhostname \ndate \nid \nwhoami \n\n\nFinal steps\n\n\n\n\n\n---## Example Submission File\nWith BOSCO you can do direct submission to the cluster, using the grid universe, or use the the glideins so that regular (vanilla) HTCondor jobs can be used.\nThere is a small difference between the 2 options is in the submit file (see below) and vanilla have some additional [[BoscoInstall#FirewallReq][Firewall and Network requirements]] because they use glideins.\nAll the other steps, job file creation, job submission and checking the jobs, are the same.\n\n\nUse one or the other\n\n\n---### Direct Job submission example\n\n\nHere is an example submission file for direct submission.  Copy it to a file, =example.condor=\n\n\nuniverse = grid\ngrid_resource = batch %RED%pbs%ENDCOLOR% marco@uc3-pbs.uchicago.edu\nExecutable     = myjob.sh\narguments = \noutput = myjob.out\nerror = myjob.error\nlog = myjob.log\ntransfer_output_files = \nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\nqueue 1\n\n\n\nThe type of cluster that you are submitting to, pbs, lsf, sge, or condor, must be supplied on the grid_resource line.\n\n\n---## Job Submission\nSubmit the job file =example.condor= with the =condor_submit= command: \n\n%UCL_PROMPT% condor_submit example.condor\n\n\n\n---## Job Monitoring\nMonitor the job with =condor_q=.  For example, the job when idle is: \n\n%UCL_PROMPT% condor_q\n\n\n-- Submitter: uc3-c001.mwt2.org : \n10.1.3.101:45876\n : uc3-c001.mwt2.org\n ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD             \n\n  12.0   marco           7/6  13:45   0+00:00:00 I  0   0.0  short2.sh 10    \n\n  13.0   marco           7/6  13:45   0+00:00:00 I  0   0.0  short2.sh 10    \n\n  14.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  15.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  16.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  17.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  18.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  19.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n\n\n8 jobs; 0 completed, 0 removed, 8 idle, 0 running, 0 held, 0 suspended\n\n\n\nNOTE\n That condor_q will show also the glidein jobs. Auxiliary jobs that BOSCO is using to run your actual job. Like in the example above, job 11 was the one submitted.\n\n\nThe job could be idle if it is currently idle at the remote cluster.  When the job is being executed on the remote cluster, the =ST= (State) will change to =R=, and the =RUN_TIME= will grow.\n\n\nAnother method of monitoring a job is to check the job's =log=, a human readable (for the most part) time line of actions relating to the job.  The =logfile= was specified in the submission script, for example =logfile= in the example above.  You can view the log file by using =cat=: \n\n%UCL_PROMPT% cat logfile\n\n\n\n---## Job output\nOnce the job completes BOSCO will transfer back standard output, standard error and the output files (if specified in the submit file), e.g. the job above will create stdout and stderr files (unique for each submission) and a file =myout-file-10= in the directory where the =condor_submit= command was executed.\n%ENDSECTION{\"BoscoJob\"}%\n\n\n%STARTSECTION{\"BoscoCommands\"}%\n---# Command summary\nUser commands:\n| \nAction\n  | \nArguments\n   | \nImplicit Input\n | \nOutput\n |\n| condor_* | Various | Various | Various see the [[http://research.cs.wisc.edu/condor/manual/][Condor manual]] | \n\n\nThere are many Condor commands. The most common user commands are =condor_q=, =condor_submit=, =condor_rm= and =condor_status= .\n\n\nAdministration commands:\n| \nAction\n  | \nArguments\n   | \nImplicit Input\n | \nOutput\n |\n| bosco_install | | | Success/Failure |\n| source bosco.[csh,sh] | | | |\n| bosco_start | | | Success/Failure |\n| bosco_stop | | | Success/Failure |\n| bosco_uninstall | | | Success/Failure |\n| bosco_cluster | --add user@Hostname queue | | Success/Fail, entry in head node table |\n| | --list | Head-node table | List of add head nodes and their status |\n| | --test Hostname | Submit file | Status of submitted jobs |\n| | --remove Hostname | | Success/Fail, head node table with Hostname removed, delete if empty |\n| condor_* | Various | Various | Various see the [[http://research.cs.wisc.edu/condor/manual/][Condor manual]] | \n\n\nManually transfer output data from batch system.\n%ENDSECTION{\"BoscoCommands\"}%\n\n\n%STARTSECTION{\"BoscoMultiCluster\"}%\n---# Multi-Cluster Bosco\n\n\nIn order to use Multi-Cluster Bosco, you must make 1 configuration change.  The multi-cluster also requires a public IP address.\n\n\n---## Changing the Bosco Configuration for Multi-Cluster\nBOSCO by default is using the loopback IP address.  You must change the configuration to listen on the public interface.  You can do this by editing the configuration file =$HOME/bosco/local.bosco/config/condor_config.factory=, adding anywhere the line: \n\nNETWORK_INTERFACE = \n\n\n\nBy setting this, you are enabling Bosco's smart interface detection which will automatically choose and listen on the public interface.\n\n\n---## Glidein Job submission example\n\n\nYou can submit a regular HTCondor vanilla job to BOSCO. The Campus Factory, a component within BOSCO, will take care to run it on the different clusters that you added and to transfer the input and output files as needed.\nHere follow a simple example. The Condor team provides [[http://research.cs.wisc.edu/condor/tutorials/][many great tutorials]] to learn more.\n\n\nHere is an example of  a vanilla submission file (using glideins).  Copy it to a file, =example.condor=\n\n\nuniverse = vanilla\nExecutable     = myjob.sh\narguments = $(Cluster) $(Process)\noutput = cfjob.out.$(Cluster)-$(Process)\nerror = cfjob.err.$(Cluster)-$(Process)\nlog = cfjob.log.$(Cluster)\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\nqueue 1\n\n\n\n%NOTE% The BOSCO submit host needs to satisfy these additional [[BoscoInstall#FirewallReq][Firewall and Network requirements]] to be able to submit and run vanilla jobs. Those requirement include being reachable by all BOSCO resources. \n\n\n%STARTSECTION{\"BoscoAdvancedUse\"}%\n---# Advanced use\n\n\n%STARTSECTION{\"BoscoAdvancedUseInstContent\"}%\n\n\n---## Changing the BOSCO port\nBOSCO is using the HTCondor [[http://research.cs.wisc.edu/htcondor/manual/latest/3_7Networking_includes.html#SECTION00472000000000000000][Shared port daemon]]. This means that all the communication are coming to the same port, by default 11000. If that port is taken (already bound), the [[BoscoQuickStart][quick start installer]] will select the first available port. You can check and edit manually the port used by BOSCO in the file =$HOME/bosco/local.bosco/config/condor_config.factory=. You can change the port passed to the shared port daemon (in %RED%red%ENDCOLOR%): \n# Enabled Shared Port\nUSE_SHARED_PORT = True\nSHARED_PORT_ARGS = -p %RED%11000%ENDCOLOR%\n\n\n%NOTE% You need to restart BOSCO after you change the configuration (=bosco_stop; bosco_start=).\n\n\nIf you are referring to this BOSCO pool (e.g. for flocking) you'll need to use a string like: =%RED%your_host.domain%ENDCOLOR%:%RED%11000%ENDCOLOR%?sock=collector= .\nReplace host and port with the correct ones.\n\n\n---## Multi homed hosts\nMulti homed hosts are hosts with multiple Network Interfaces (aka dual-homed when they have 2 NICs).\nBOSCO configuration is tricky on multi-homed hosts. BOSCO requires the submit host to be able to connect back to the BOSCO host, so it must advertise an interface that is reachable from all the chosen submit hosts. E.g. a host with a NIC on a private network and one with a public IP address must advertise the public address if the submit hosts are outside of the private network. \nIn order to do that you have to:\n   * make sure that the name returned by the command =/bin/hostname -f= is the name resolving in the public address (e.g. =host \nhostname -f\n= should return the public address). If not you should change it.\n   * edit =~/bosco/local.%RED%$HOST%ENDCOLOR%/condor_config.local= (HOST is the short host name) and add a line like =NETWORK_INTERFACE = xxx.xxx.xxx.xxx= , substituting xxx.xxx.xxx.xxx with the public IP address. This will tell BOSCO to use that address.\n\n\n---## Modifying maximum number of submitted jobs to a resource\n\n\nMany clusters limit the number of jobs that can be submitted to the scheduler.  For PBS, we are able to detect this limit.  For SGE and LSF, we are not able to detect this limit.  In the cases where we cannot find the limit, we set the maximum number of jobs very conservatively, to a maximum of 10.  This includes both the number of idle and running jobs to the cluster.\n\n\nThe limit is specified in the condor config file =~/bosco/local.bosco/condor_config.local=, at the bottom.  Edit the value of the configuration variable =GRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE=\n\n\n\nGRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE = %RED%10%ENDCOLOR%\n\n\n\n\n---## Custom submit properties\nBosco has the ability to add custom submit properties to every job submitted to a cluster.  On the cluster's login node (the BOSCO resource, the host you used at the end of the line when typing the =bosco_cluster --add= command), create the file \n\n\nCustomScriptLocations\n\n\n\n\nPBS/SLURM\n - =~/bosco/glite/bin/pbs_local_submit_attributes.sh=\n\n\nCondor\n - =~/bosco/glite/bin/condor_local_submit_attributes.sh=\n\n\nSGE\n (and other GE) - =~/bosco/glite/bin/sge_local_submit_attributes.sh=\n\n\nLSF\n - =~/bosco/glite/bin/lsf_local_submit_attributes.sh=\n\n\n\n\n%IMPORTANT% This file is executed and the output is inserted into the submit script. I.e. It is not cat, use echo/cat statements in the script.\n\n\nBelow is an example =pbs_local_submit_attributes.sh= script which will cause every job submitted to this cluster through Bosco to request 1 node with 8 cores:\n\n\n\n!/bin/sh\n\n\necho \"#PBS -l nodes=1:ppn=8\"\n\n\n\n---### Passing parameters to the custom submit properties.\nYou may also pass parameters to the custom scripts by adding a special parameter to the Bosco submit script.\n\n\nFor example, in your Bosco submit script, add: \n\n...\n%RED%+remote_cerequirements = NumJobs == 100%ENDCOLOR%\n...\nqueue\n\n\n\nAfter you submit this job to Bosco, it will execute the [[#CustomScriptLocations][custom scripts]] with, in this example, =NumJobs= set in the environment equal to =100=.  The custom script can take advantage of these values.  For example, a PBS script can use the !NumJobs: \n\n\n!/bin/sh\n\n\necho \"#PBS -l select=$NumJobs\"\n\n\n\nThis will set the number of requested cores from PBS to !NumJobs specified in the original Bosco Submit file.\n\n\n---## Flocking to a BOSCO installation\nIn some special cases you may desire to flock to your BOSCO installation. If you don't know what I'm talking about, then skip this section.\n\n\nIn order to enable flocking you must use an IP so that all the hosts you are flocking from can communicate with the BOSCO host.\nThen you must setup FLOCK_FROM and the security configuration so that the communications are authorized.\n\n\nBOSCO has strong security settings. Here are two examples:\n   1 Using GSI authentication (a strong authentication method) you must provide and install X509 certificates, you must change the configuration %TWISTY{%TWISTY_OPTS_DETAILED% showlink=\"Click to see the configuration file\" }%   \n#\n\n\nNetworking - If you did not already, remember that you need to set BOSCO not to use the loopback port\n\n\n\n\nNETWORK_INTERFACE =\n\n\n\n\nHosts definition\n\n\n\n\nBOSCO host\n\n\nH_BOSCO = %RED%bosco.mydomain.edu%ENDCOLOR%\nH_BOSCO_DN = %RED%/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=bosco.mydomain.edu%ENDCOLOR%\n\n\nsubmit host (flocking to BOSCO host)\n\n\nH_SUB = %RED%sub.mydomain.edu%ENDCOLOR%\nH_SUB_DN = %RED%/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=sub.mydomain.edu%ENDCOLOR%\n\n\n\n\nFlocking configuration\n\n\n\n\nFLOCK_FROM = $(FLOCK_FROM) $(H_SUB)\n\n\n\n\nSecurity definitions\n\n\n\n\nAssuming system-wide installed CA certificates\n\n\nGSI_DAEMON_DIRECTORY = /etc/grid-security\n\n\nThis host's certificates\n\n\nGSI_DAEMON_CERT = /etc/grid-security/hostcert.pem\nGSI_DAEMON_KEY = /etc/grid-security/hostkey.pem\n\n\ndefault GSI_DAEMON_TRUSTED_CA_DIR = $(GSI_DAEMON_DIRECTORY)/certificates\n\n\nCERTIFICATE_MAPFILE= $HOME/bosco/local.bosco/certs/condor_mapfile\n\n\nNot used\n\n\nMY_DN = $(H_BOSCO_DN)\n\n\nWho to trust?  Include the submitters flocking here\n\n\nGSI_DAEMON_NAME = $(GSI_DAEMON_NAME), $(H_BOSCO_DN), $(H_SUB_DN)\n\n\nEnable authentication from the Negotiator\n\n\nSEC_ENABLE_MATCH_PASSWORD_AUTHENTICATION = TRUE\n\n\nEnable gsi authentication, and claimtobe (for campus factories)\n\n\nThe default (unix) should be: FS, KERBEROS, GSI\n\n\nSEC_DEFAULT_AUTHENTICATION_METHODS = FS,GSI, PASSWORD, $(SEC_DEFAULT_AUTHENTICATION_METHODS)\nSEC_CLIENT_AUTHENTICATION_METHODS = FS, PASSWORD, GSI, CLAIMTOBE\nSEC_DAEMON_AUTHENTICATION_METHODS = FS, PASSWORD, GSI, CLAIMTOBE\nSEC_WRITE_AUTHENTICATION_METHODS = FS, PASSWORD, GSI, CLAIMTOBE\nSEC_ADVERTISE_SCHEDD_METHODS = FS, PASSWORD, GSI, CLAIMTOBE\n\n\nALLOW_DAEMON = $(ALLOW_DAEMON) condor_pool@\n/\n %RED%boscouser%ENDCOLOR%@\n/\n $(FULL_HOSTNAME) $(IP_ADDRESS)\nALLOW_ADVERTISE_SCHEDD = %RED%boscouser%ENDCOLOR%@\n/\n\n\n %ENDTWISTY% and define or update the condor_mapfile (e.g. =$HOME/bosco/local.bosco/certs/condor_mapfile=) %TWISTY{%TWISTY_OPTS_DETAILED% showlink=\"Click to see the condor_mapfile\" }%   \n#\nGSI \"^%RED%\\/DC\\=com\\/DC\\=DigiCert-Grid\\/O\\=Open\\ Science\\ Grid\\/OU\\=Services\\/CN\\=sub.mydomain.edu%ENDCOLOR%$\" %RED%boscouser@sub.mydomain.edu%ENDCOLOR%\nGSI \"^%RED%\\/DC\\=com\\/DC\\=DigiCert-Grid\\/O\\=Open\\ Science\\ Grid\\/OU\\=Services\\/CN\\=bosco.mydomain.edu%ENDCOLOR%$\" %RED%boscouser%ENDCOLOR%\n\n\n\n\nSSL (.\n) ssl@unmapped\nCLAIMTOBE (.\n) \\1\nPASSWORD (.*) \\1\n\n\n\n\nGSI (.\n) anonymous\nFS (.\n) \\1\n\n %ENDTWISTY% Remember to enable and configure GSI authentication also on the host you are flocking form.\n   1 Relaxing BOSCO security setting to allow CLAIMTOBE authentication. This is not very secure. Use it only if you can trust all the machines on the network and remember to enable CLAIMTOBE also on the host you are flocking from %TWISTY{%TWISTY_OPTS_DETAILED% showlink=\"Click to see the configuration file\" }%   \n#\n\n\nNetworking - If you did not already, remember that you need to set BOSCO not to use the loopback port\n\n\n\n\nNETWORK_INTERFACE =\n\n\n\n\nFlocking configuration\n\n\n\n\nFLOCK_FROM = %RED%host_from.domain%ENDCOLOR%\n\n\n\n\nSecurity definitions overrides\n\n\n\n\nSEC_DEFAULT_ENCRYPTION = OPTIONAL\nSEC_DEFAULT_INTEGRITY = PREFERRED\n\n\nTo allow status read\n\n\nSEC_READ_INTEGRITY = OPTIONAL\n\n\nSEC_CLIENT_AUTHENTICATION_METHODS = FS, PASSWORD, CLAIMTOBE\n\n\nALLOW_ADVERTISE_SCHEDD = */%RED%IP_of_the_host_in_flock_from%ENDCOLOR% $(FULL_HOSTNAME) $(IP_ADDRESS) $(ALLOW_DAEMON)\n\n\nSEC_DAEMON_AUTHENTICATION = PREFERRED\nSEC_DAEMON_INTEGRITY = PREFERRED\nSEC_DAEMON_AUTHENTICATION_METHODS = FS,PASSWORD,CLAIMTOBE\nSEC_WRITE_AUTHENTICATION_METHODS = FS,PASSWORD,CLAIMTOBE\n\n %ENDTWISTY%\n\n\nAfter copying from the examples (click above to expand the example files) or editing your configuration file, save it as =$HOME/bosco/local.bosco/config/zzz_condor_config.flocking=. \nOther names are OK as long as its definition override the default ones of BOSCO (check with =condor_config_val -config=).\n\n\nThen stop and restart BOSCO.\n\n\n%ENDSECTION{\"BoscoAdvancedUseInstContent\"}%\n%ENDSECTION{\"BoscoAdvancedUse\"}%\n\n\n---# Troubleshooting\n\n\n---## Useful Configuration and Log Files\nBOSCO underneath is using Condor. You can find all the Condor log files in =~/bosco/local.HOSTNAME/log=\n\n\n%STARTSECTION{\"BoscoTroubleshootingItems\"}%\n---## Make sure that you can connect to the BOSCO host\nIf you see errors like:\n\n Installing BOSCO on user@osg-ss-submit.chtc.wisc.edu...\n ssh: connect to host osg-ss-submit.chtc.wisc.edu port 22: Connection timed out\n rsync: connection unexpectedly closed (0 bytes received so far) [sender]\n rsync error: unexplained error (code 255) at io.c(600) [sender=3.0.6]\n ssh: connect to host osg-ss-submit.chtc.wisc.edu port 22: Connection timed out\n rsync: connection unexpectedly closed (0 bytes received so far) [sender]\n rsync error: unexplained error (code 255) at io.c(600) [sender=3.0.6]\n ssh: connect to host osg-ss-submit.chtc.wisc.edu port 22: Connection timed out\n rsync: connection unexpectedly closed (0 bytes received so far) [sender]\n rsync error: unexplained error (code 255) at io.c(600) [sender=3.0.6]\n\n\nPlease try manually to ssh from the BOSCO host to the cluster submit node. The ability to connect is required in order to install BOSCO.\n\n\n---## Make sure that BOSCO is running\nBOSCO may not survive after you log out, for example if the BOSCO node was restarted while you where logged out. \nWhen you log back in after sourcing the setup as described in the [[#SetupEnvironment][setup environment section]], you should start BOSCO as described in the [[#BoscoStart][BOSCO start section]], specially if the command =condor_q= is failing.\n\n\n---## Errors due to leftover files\nBosco files on the submit host are in:\n   * =~/bosco/= - the release directory\n   * =~/.bosco/= - some service files\n   * =~/.ssh/= - the ssh key used by BOSCO\n\n\nIf you used =bosco_uninstall= it will remove all BOSCO related files. If you removed BOSCO by hand you must pay attention.\nIf the service key is still in =.ssh= but the other files are missing, during the execution of BOSCO commands you will get some unclear errors like \n\n\"IOError: [Errno 2] No such file or directory: '/home/marco/.bosco/.pass'\"\n , \n\"OSError: [Errno 5] Input/output error\"\n , all followed by:\n\nPassword-less ssh to marco@itb2.uchicago.edu did NOT work, even after adding the ssh-keys.\nDoes the remote resource allow password-less ssh?\n\n\n\nIf that happens you can remove the service files and the keys using:\nrm -r ~/.bosco\nrm ~/.ssh/bosco_key.rsa*\n\nand then re-add all the clusters with =bosco_cluster --add=.\n\n\n---## Unable to download and prepare BOSCO for remote installation. \nBOSCO can return this error:\n   1. Because the BOSCO submit host is unable to download BOSCO for the resource installation, e.g. a firewall is blocking the download or the server is down\n   2. More commonly because there are problems with the login host of the BOSCO resource, e.g. the disk is full or there are multiple login nodes\nYou can check 1 byy downloading BOSCO on your BOSCO submit host.\nTo check 2 you have to login on the BOSCO resource: =df= will tell you you some disks are full, with =hostname -f= you can check if the name is different form the one that you used to login with ssh. If the name differs probably you are using a cluster with multiple login nodes and you must use only one for BOSCO. Se the second \"IMPORTANT\" note in the [[#AddResourceSection][section to add a cluster to BOSCO]] (above).\n\n\nIf you see errors similar to the one below while executing ==bosco_cluster --add==:\n\n\nDownloading for USER@RESOURCE\nUnpacking.tar: Cannot save working directory \ntar: Error is not recoverable: exiting now \nls: /tmp/tmp.qeIJ9139/condor*: No such file or directory \nUnable to download and prepare BOSCO for remote installation. \n\n\nthen you are using most likely the generic name of a multi-login cluster and you should use the name of one of the nodes as suggested in the [[#AddResourceSection][note above]]. \n\n\n%ENDSECTION{\"BoscoTroubleshootingItems\"}%\n\n\n---# Get Help/Support\nTo get assistance you can send an email to bosco-discuss@opensciencegrid.org\n\n\n%STARTSECTION{\"BoscoReferences\"}%\n---# References \n[[http://bosco.opensciencegrid.org/][BoSCO Web site]] and documents about the latest production release (v1.2)\n   * [[BoSCO][Using Bosco]]\n   * [[BoscoInstall][Installing BoSCO]]\n   * [[BoscoMultiUser][Installing BoSCO Multi User]]\n   * [[BoscoQuickStart][Quick start guide to Bosco]]\n   * BoscoR\n\n\nCampus Grids related documents:\n   * https://twiki.grid.iu.edu/bin/view/CampusGrids\n   * https://twiki.grid.iu.edu/bin/view/Documentation/CampusFactoryInstall\n\n\nCondor documents:\n   * Condor manual: http://research.cs.wisc.edu/condor/manual/\n\n\nHow to submit Condor jobs:\n   * Tutorial: http://research.cs.wisc.edu/condor/tutorials/alliance98/submit/submit.html\n   * Condor manual: http://research.cs.wisc.edu/condor/manual/v7.6/2_5Submitting_Job.html\n\n\nDevelopers documents:\n   * [[TestBoSCO][BOSCO tests for developers and testers]]\n   * [[BoscoRoadmap][BOSCO Roadmap (planned and desired features)]]\n\n\nHere you can check out older releases:\n   * [[BoSCOv0][BOSCO version 0]]\n   * [[BoSCOv1][BOSCO version 1]]\n   * [[BoSCOv1p1][BOSCO version 1.1]]\n   * [[BoSCOv1p2][BOSCO version 1.2]]\n\n\n\n\nCICiForum130418", 
            "title": "Bosco Installation"
        }, 
        {
            "location": "/BoscoInstall/#bosco-installation", 
            "text": "", 
            "title": "Bosco Installation"
        }, 
        {
            "location": "/BoscoInstall/#introduction", 
            "text": "BOSCO is a job\nsubmission manager designed to help researchers manage large numbers\n(\\~1000s) of job submissions to the different resources that they can\naccess on a campus. This is release 1.2 of BOSCO, if you find any\nproblems or need help installing or running Bosco, please email  .  It offers the following capabilities:   Jobs are automatically resubmitted when they fail. The researcher\n    does not need to babysit their jobs.  Job submissions can be throttled to meet batch scheduler\n    settings (e.g. only 10 jobs running concurrently). The researcher\n    does not need to make multiple submissions. BOSCO handles that\n    for them.  BOSCO is designed to be flexible and allows jobs to be submitted to\n    multiple clusters, with different job schedulers (e.g. PBS, LSF,\n    SGE, HTCondor, SLURM*).   The primary advantage for the researcher is that they only need to learn\none job scheduler environment even if the clusters utilize different\nnative environments.  * SLURM support is via its PBS emulation  This document explains how to install, configure and use BOSCO for a\nsingle user. We recommend to use the  Bosco Quick Start  guide (less\nflexible but easier and guided setup), if you plan to install BOSCO only\nfor you (single user) and to connect it to only one cluster. Bosco Quick Start  will give you a full installation but to learn how to\nconnect to multiple resources you have to read this document, it is not\nexplained in the quick start guide.", 
            "title": "Introduction"
        }, 
        {
            "location": "/BoscoInstall/#requirements", 
            "text": "BOSCO Submit-node \n:   This is the system that the researcher uses to submit jobs. In\n    general it can be the user's laptop, workstation, or it can be\n    another system that the user logs into for submitting jobs to\n    the cluster.  There can not be any Condor collector running on the\n    submit node , otherwise it will conflict with Bosco.    BOSCO resource (aka Cluster) submit-node \n:   This is the node that you normally login to on the PBS, SGE, LSF,\n    SLURM or HTCondor cluster (the BOSCO resource that you'd like\n    to add).    PBS flavors supported \n:   Torque and PBSPro    HTCondor flavors supported \n:   HTCondor 7.6 or later    SGE flavors supported \n:   no special requirements (Sun Grid Engine and other Grid Engine\n    versions supported)    LSF flavors \n:   no special requirements    SLURM flavors \n:   no special requirements    BOSCO resource (aka Cluster) \n:   This is the remote cluster that jobs will execute on (the\n    BOSCO resource). The Cluster submit-node is a node belonging to\n    this cluster. The nodes where jobs run are referred as worker nodes.\n    All the cluster needs:    Shared Filesystem \n    :   The Cluster needs a shared home filesystem (if the Cluster has         no shared filesystem only Grid universe jobs can be sent to it)    Network Access \n    :   The worker nodes need to have access to the submit host. The         worker nodes can be behind a          NAT          between the worker nodes and the submit host.     Note  The BOSCO resource requirements\njust listed (shared file system and outbound network access) are\nrequired only if you submit jobs using the HTCondor  vanilla  universe.\nIf you submit jobs using the  grid universe , submitting to one\nresource at the time, then both requirements can be relaxed. I.e. there\nis  no need of a shared file system or access to the BOSCO submit host\nfrom the BOSCO resource .   BOSCO can be used as part of a more complex Condor setup (with flocking\nor multiple pools). Whatever the setup:   the BOSCO host needs connectivity to the cluster submit nodes of the\n    BOSCO resources  the worker nodes of the BOSCO resources (running the jobs, e.g. the\n    nodes in the PBS cluster) must have network connectivity to the jobs\n    submit node (the BOSCO host or a different Condor schedd flocking\n    into it)", 
            "title": "Requirements"
        }, 
        {
            "location": "/BoscoInstall/#networking", 
            "text": "", 
            "title": "Networking"
        }, 
        {
            "location": "/BoscoInstall/#bosco-submit-host", 
            "text": "The BOSCO submit host requires no ports open for  universe = grid  jobs.  For  universe = vanilla  jobs, port 11000 is required.   Note   The port 11000 is required ONLY  if \"vanilla\" jobs are submitted. \"vanilla\" jobs require also\nthat all worker nodes be able to reach the submit host: this means that\nthe submit host has to have a public IP address or at least an address\nknown by all BOSCO resources that are going to be added. If you need to\nuse a port different from 11000, you can edit the BOSCO configuration.", 
            "title": "BOSCO submit host"
        }, 
        {
            "location": "/BoscoInstall/#bosco-resources", 
            "text": "The cluster login node requires port 22 from the BOSCO submit host.  BOSCO will\nuse the SSH protocol to communicate with the login node.  The second requirement is for the BOSCO worker nodes (wherever the jobs\nwill run) and it is needed  only if \"vanilla\" jobs are used .  The worker nodes must\nbe able to connect to the BOSCO submit host.  Only outgoing from the worker node to the \nBOSCO submit host is required.", 
            "title": "BOSCO Resources"
        }, 
        {
            "location": "/BoscoInstall/#how-to-install", 
            "text": "", 
            "title": "How to Install"
        }, 
        {
            "location": "/BoscoInstall/#installation-procedure", 
            "text": "Download  Bosco from\n    the bosco website. We recommend the  Multi-Platform  installer. It\n    is a bit more complex than the  quick start\n    installation  but allows more options.  Untar Bosco from the command line:  pre class=\"screen\"   $ cd Downloads  \n  $ tar xzf boscoinstaller.tar.gz   Run the installer:   $ python boscoinstaller", 
            "title": "Installation Procedure"
        }, 
        {
            "location": "/BoscoInstall/#how-to-use", 
            "text": "Now BOSCO is installed. To use it:\n   1. Setup the environment\n   1. Add all the desired clusters (at least one)\n   1. Start BOSCO\n   1. Submit a test job\n   1. Ready to submit a real job", 
            "title": "How to Use"
        }, 
        {
            "location": "/BoscoInstall/#starting-bosco", 
            "text": "BOSCO has some persistent services that must be running. You'll have to start it at the beginning and probably after each reboot of your host.\nYou should stop BOSCO before an upgrade and possibly before a shutdown of your host.\nIf you will not use BOSCO anymore,  bosco_uninstall  will remove it from your system.  To start BOSCO:  $ bosco_start", 
            "title": "Starting BOSCO"
        }, 
        {
            "location": "/BoscoInstall/#add-a-cluster-to-bosco", 
            "text": "To add a new cluster to the resources you will be using through BOSCO:   Setup the environment appropriate for your shell as described in the  setup environment section  (above).  For the cluster  mycluster  with user  username  and submit host  mycluster-submit.mydomain.org  (Fully Qualified Domain Name, aka full hostname including the domain name) and Local Resource Management System  LRMS  (valid values are =pbs=, =condor=, =sge= or =lsf=). Replace the parts in red:  \n%UCL_PROMPT% bosco_cluster --add %RED%username@mycluster-submit.mydomain.org LRMS%ENDCOLOR%   %TWISTY{%TWISTY_OPTS_DETAILED% }%    \n-bash-3.2$ bosco_cluster -add itbv-ce-pbs.uchicago.edu\nEnter password to copy ssh keys to itbv-ce-pbs.uchicago.edu:\nThe authenticity of host 'itbv-ce-pbs.uchicago.edu (128.135.158.176)' can't be established.\nRSA key fingerprint is 8e:a6:db:18:80:6b:b7:de:56:c8:5a:a2:75:19:11:8d.\nAre you sure you want to continue connecting (yes/no)? yes\nWarning: Permanently added 'itbv-ce-pbs.uchicago.edu,128.135.158.176' (RSA) to the list of known hosts.\nInstalling BOSCO on itbv-ce-pbs.uchicago.edu...\nInstallation complete  %ENDTWISTY%   When you add your first cluster, BOSCO will prompt you for a password that will be used to store the SSH keys used by BOSCO to access all your clusters (=Enter password for bosco ssh key:=). Select a random string. It is preferable if you do not use the password you use to access the cluster or to unlock your SSH keys.  Then, if you don't have a ssh key agent with that cluster enabled, you will be prompted for the password that you use to access the cluster you are adding to BOSCO (=Enter password to copy ssh keys to ...=). This may be followed by a confirmation of the RSA key fingerprint, if it is your first ssh connection from this host, where you have to answer =yes=.  %IMPORTANT% You must be able to login to the remote cluster. If password authentication is OK, the script will ask you for your password. If key only login is allowed, then you must load your key in the =ssh-agent=. Here is an example adding the key and testing the login: %TWISTY{%TWISTY_OPTS_DETAILED% }%    \n%UCL_PROMPT% eval  ssh-agent \nAgent pid 17103;\n%UCL_PROMPT% ssh-add id_rsa_bosco \nEnter passphrase for id_rsa_bosco: \nIdentity added: id_rsa_bosco (id_rsa_bosco)\n%UCL_PROMPT% ssh  uc3@itbv-ce-pbs\nLast login: Thu Sep 13 13:49:33 2012 from uc3-bosco.mwt2.org\n$ logout \n%ENDTWISTY%  %IMPORTANT% Some clusters have multiple login nodes behind a round robin DNS server. You can recognize them because when you login to the node (e,g: ==ssh login.mydomain.org==), it will show a name different form the one used to connect (e.g.: ==hostname -f== will return ==login2.mydomain.org==). If this happens you must add the BOSCO resources by using a name of the host, not the DNS alias (e.g. ==bosco_cluster --add login2.mydomain.org==). This is because sometime these multiple login nodes do not share all the directories and BOSCO may be unable to find its files if different connections land on different hosts : %TWISTY{%TWISTY_OPTS_DETAILED% }%  Note how =midway-login2.rcc.uchicago.edu= is use instead of =midway.rcc.uchicago.edu=:  \n%UCL_PROMPT% ssh  mm@%RED%midway.rcc.uchicago.edu%ENDCOLOR%\nmm@midway.rcc.uchicago.edu's password: \n===============================================================================\n                               Welcome to Midway\n                           Research Computing Center\n                             University of Chicago\n...\n===============================================================================  [mm@midway-login2 ~]$ hostname -f\n%RED%midway-login2.rcc.uchicago.edu%ENDCOLOR%\n[mm@midway-login2 ~]$ logout\nConnection to midway.rcc.uchicago.edu closed.\n%UCL_PROMPT% marco$ bosco_cluster --add mm@%RED%midway-login2.rcc.uchicago.edu%ENDCOLOR%\nWarning: No batch system specified, defaulting to PBS\nIf this is incorrect, rerun the command with the batch system specified  Enter the password to copy the ssh keys to mm@midway-login2.rcc.uchicago.edu:\nmm@midway-login2.rcc.uchicago.edu's password: \nDetecting PBS cluster configuration...bash: cannot set terminal process group (-1): Invalid argument\nbash: no job control in this shell\nbash: qmgr: command not found\nDone!\nDownloading for mm@midway-login2.rcc.uchicago.edu....\nUnpacking.........\nSending libraries to mm@midway-login2.rcc.uchicago.edu...\nCreating BOSCO for the WN's....................................................................\nInstallation complete\nThe cluster mm@midway-login2.rcc.uchicago.edu has been added to BOSCO\nIt is available to run jobs submitted with the following values:   universe = grid\ngrid_resource = batch pbs mm@midway-login2.rcc.uchicago.edu\n%UCL_PROMPT% \n%ENDTWISTY%   When adding the cluster, if the last message is =Done!=. Your cluster has been added successfully.  You can see a list of the current clusters in BOSCO by typing: \n%UCL_PROMPT% bosco_cluster --list \n%TWISTY{%TWISTY_OPTS_DETAILED% }%    \n-bash-3.2$ bosco_cluster --list\nitbv-ce-pbs.uchicago.edu \n%ENDTWISTY%\n%ENDSECTION{\"BoscoAddResource\"}%  ---## Submitting a test job\nYou can send a simple test job to verify that the cluster added is working correctly.  To send a BOSCO test job to the host %RED%username@mycluster-submit.mydomain%ENDCOLOR% (name as listed in the output of =bosco_cluster --list=):\n   1. Setup the environment appropriate for your shell as described in the setup environment section (above).\n   1. For the cluster %RED%username@mycluster-submit.mydomain%ENDCOLOR% (identical to output of  =bosco_cluster --list=). Replace the parts in red:  \n%UCL_PROMPT% bosco_cluster --test %RED%username@mycluster-submit.mydomain%ENDCOLOR%  %TWISTY{%TWISTY_OPTS_DETAILED% }%    \n%UCL_PROMPT% $ bosco_cluster -t dweitzel@ff-grid.unl.edu\ndweitzel@ff-grid.unl.edu\nTesting ssh to dweitzel@ff-grid.unl.edu...Passed!\nTesting bosco submission...Passed!\nChecking for submission to remote pbs cluster (could take ~30 seconds)...Passed!\nSubmission files for these jobs are in /home/dweitzel/bosco/local.localhocentos56/bosco-test\nExecution on the remote cluster could take a while...Exiting  %ENDTWISTY%\n%ENDSECTION{\"BoscoUseShort\"}%  ---## How to Stop and Remove\nTo stop BOSCO: \n%UCL_PROMPT% bosco_stop  To uninstall BOSCO:\n   * If you want to remove remote clusters get the list and remove them one by one:  %UCL_PROMPT% bosco_cluster --list", 
            "title": "Add a cluster to BOSCO"
        }, 
        {
            "location": "/BoscoInstall/#for-each-remote-cluster", 
            "text": "%UCL_PROMPT% bosco_cluster -r %RED%user@cluster_as_spelled_in_list%ENDCOLOR% \n   * Remove the installation directory:   %UCL_PROMPT% bosco_uninstall  %NOTE% Uninstalling BOSCO removes the software but leaves the files in your =.bosco= and =.ssh= directories with all the information about the added clusters and the SSH keys. Files installed on the remote clusters are not removed either.   ---## How to Update BOSCO\nIf you want to update BOSCO to a new version you have to:\n   1. setup BOSCO: %UCL_PROMPT% source ~/bosco/bosco_setenv \n   1. stop BOSCO:  %UCL_PROMPT% bosco_stop \n   1. remove the old BOSCO:  %UCL_PROMPT% bosco_uninstall \n   1. download and install the new BOSCO (see install section above)  and re-add all the clusters in your setup:\n   1. for each installed cluster (the list is returned by =bosco_cluster --list=):\n      1. remove the cluster:  %UCL_PROMPT% bosco_cluster --remove %RED%username@mycluster-submit.mydomain.org%ENDCOLOR%  \n      1. add the cluster:  %UCL_PROMPT% bosco_cluster --add %RED%username@mycluster-submit.mydomain.org queue%ENDCOLOR%  \n   1. start BOSCO:  %UCL_PROMPT% bosco_start  This will update the local installation and the software on the remote clusters\n%ENDSECTION{\"BoscoSetup\"}%  %STARTSECTION{\"BoscoJob\"}%\n---# Job submission example  You can submit a regular Condor vanilla job to BOSCO. The Campus Factory, a component within BOSCO, will take care to run it on the different clusters that you added and to transfer the input and output files as needed.\nHere is a simple example. The Condor team provides [[http://research.cs.wisc.edu/condor/tutorials/][many great tutorials]] to learn more.  ---## Configuring Executable\nYour may wrap your job in a script (e.g. using your favorite shell or Python) so that you can invoke more executables and do some tests.  This is a simple bash script, called =myjob.sh=:", 
            "title": "For each remote cluster"
        }, 
        {
            "location": "/BoscoInstall/#binbash", 
            "text": "", 
            "title": "!/bin/bash"
        }, 
        {
            "location": "/BoscoInstall/#prepare-for-the-execution", 
            "text": "", 
            "title": "Prepare for the execution"
        }, 
        {
            "location": "/BoscoInstall/#run-the-actual-applications", 
            "text": "hostname \ndate \nid \nwhoami", 
            "title": "Run the actual applications"
        }, 
        {
            "location": "/BoscoInstall/#final-steps", 
            "text": "---## Example Submission File\nWith BOSCO you can do direct submission to the cluster, using the grid universe, or use the the glideins so that regular (vanilla) HTCondor jobs can be used.\nThere is a small difference between the 2 options is in the submit file (see below) and vanilla have some additional [[BoscoInstall#FirewallReq][Firewall and Network requirements]] because they use glideins.\nAll the other steps, job file creation, job submission and checking the jobs, are the same.  Use one or the other  ---### Direct Job submission example  Here is an example submission file for direct submission.  Copy it to a file, =example.condor= \nuniverse = grid\ngrid_resource = batch %RED%pbs%ENDCOLOR% marco@uc3-pbs.uchicago.edu\nExecutable     = myjob.sh\narguments = \noutput = myjob.out\nerror = myjob.error\nlog = myjob.log\ntransfer_output_files = \nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\nqueue 1  The type of cluster that you are submitting to, pbs, lsf, sge, or condor, must be supplied on the grid_resource line.  ---## Job Submission\nSubmit the job file =example.condor= with the =condor_submit= command:  \n%UCL_PROMPT% condor_submit example.condor  ---## Job Monitoring\nMonitor the job with =condor_q=.  For example, the job when idle is:  \n%UCL_PROMPT% condor_q  -- Submitter: uc3-c001.mwt2.org :  10.1.3.101:45876  : uc3-c001.mwt2.org\n ID      OWNER            SUBMITTED     RUN_TIME ST PRI SIZE CMD              \n  12.0   marco           7/6  13:45   0+00:00:00 I  0   0.0  short2.sh 10     \n  13.0   marco           7/6  13:45   0+00:00:00 I  0   0.0  short2.sh 10     \n  14.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  15.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  16.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  17.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  18.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh\n  19.0   marco           7/6  13:46   0+00:00:00 I  0   0.0  glidein_wrapper.sh  8 jobs; 0 completed, 0 removed, 8 idle, 0 running, 0 held, 0 suspended  NOTE  That condor_q will show also the glidein jobs. Auxiliary jobs that BOSCO is using to run your actual job. Like in the example above, job 11 was the one submitted.  The job could be idle if it is currently idle at the remote cluster.  When the job is being executed on the remote cluster, the =ST= (State) will change to =R=, and the =RUN_TIME= will grow.  Another method of monitoring a job is to check the job's =log=, a human readable (for the most part) time line of actions relating to the job.  The =logfile= was specified in the submission script, for example =logfile= in the example above.  You can view the log file by using =cat=:  \n%UCL_PROMPT% cat logfile  ---## Job output\nOnce the job completes BOSCO will transfer back standard output, standard error and the output files (if specified in the submit file), e.g. the job above will create stdout and stderr files (unique for each submission) and a file =myout-file-10= in the directory where the =condor_submit= command was executed.\n%ENDSECTION{\"BoscoJob\"}%  %STARTSECTION{\"BoscoCommands\"}%\n---# Command summary\nUser commands:\n|  Action   |  Arguments    |  Implicit Input  |  Output  |\n| condor_* | Various | Various | Various see the [[http://research.cs.wisc.edu/condor/manual/][Condor manual]] |   There are many Condor commands. The most common user commands are =condor_q=, =condor_submit=, =condor_rm= and =condor_status= .  Administration commands:\n|  Action   |  Arguments    |  Implicit Input  |  Output  |\n| bosco_install | | | Success/Failure |\n| source bosco.[csh,sh] | | | |\n| bosco_start | | | Success/Failure |\n| bosco_stop | | | Success/Failure |\n| bosco_uninstall | | | Success/Failure |\n| bosco_cluster | --add user@Hostname queue | | Success/Fail, entry in head node table |\n| | --list | Head-node table | List of add head nodes and their status |\n| | --test Hostname | Submit file | Status of submitted jobs |\n| | --remove Hostname | | Success/Fail, head node table with Hostname removed, delete if empty |\n| condor_* | Various | Various | Various see the [[http://research.cs.wisc.edu/condor/manual/][Condor manual]] |   Manually transfer output data from batch system.\n%ENDSECTION{\"BoscoCommands\"}%  %STARTSECTION{\"BoscoMultiCluster\"}%\n---# Multi-Cluster Bosco  In order to use Multi-Cluster Bosco, you must make 1 configuration change.  The multi-cluster also requires a public IP address.  ---## Changing the Bosco Configuration for Multi-Cluster\nBOSCO by default is using the loopback IP address.  You must change the configuration to listen on the public interface.  You can do this by editing the configuration file =$HOME/bosco/local.bosco/config/condor_config.factory=, adding anywhere the line:  \nNETWORK_INTERFACE =   By setting this, you are enabling Bosco's smart interface detection which will automatically choose and listen on the public interface.  ---## Glidein Job submission example  You can submit a regular HTCondor vanilla job to BOSCO. The Campus Factory, a component within BOSCO, will take care to run it on the different clusters that you added and to transfer the input and output files as needed.\nHere follow a simple example. The Condor team provides [[http://research.cs.wisc.edu/condor/tutorials/][many great tutorials]] to learn more.  Here is an example of  a vanilla submission file (using glideins).  Copy it to a file, =example.condor= \nuniverse = vanilla\nExecutable     = myjob.sh\narguments = $(Cluster) $(Process)\noutput = cfjob.out.$(Cluster)-$(Process)\nerror = cfjob.err.$(Cluster)-$(Process)\nlog = cfjob.log.$(Cluster)\nshould_transfer_files = YES\nwhen_to_transfer_output = ON_EXIT\nqueue 1  %NOTE% The BOSCO submit host needs to satisfy these additional [[BoscoInstall#FirewallReq][Firewall and Network requirements]] to be able to submit and run vanilla jobs. Those requirement include being reachable by all BOSCO resources.   %STARTSECTION{\"BoscoAdvancedUse\"}%\n---# Advanced use  %STARTSECTION{\"BoscoAdvancedUseInstContent\"}%  ---## Changing the BOSCO port\nBOSCO is using the HTCondor [[http://research.cs.wisc.edu/htcondor/manual/latest/3_7Networking_includes.html#SECTION00472000000000000000][Shared port daemon]]. This means that all the communication are coming to the same port, by default 11000. If that port is taken (already bound), the [[BoscoQuickStart][quick start installer]] will select the first available port. You can check and edit manually the port used by BOSCO in the file =$HOME/bosco/local.bosco/config/condor_config.factory=. You can change the port passed to the shared port daemon (in %RED%red%ENDCOLOR%):  # Enabled Shared Port\nUSE_SHARED_PORT = True\nSHARED_PORT_ARGS = -p %RED%11000%ENDCOLOR% \n%NOTE% You need to restart BOSCO after you change the configuration (=bosco_stop; bosco_start=).  If you are referring to this BOSCO pool (e.g. for flocking) you'll need to use a string like: =%RED%your_host.domain%ENDCOLOR%:%RED%11000%ENDCOLOR%?sock=collector= .\nReplace host and port with the correct ones.  ---## Multi homed hosts\nMulti homed hosts are hosts with multiple Network Interfaces (aka dual-homed when they have 2 NICs).\nBOSCO configuration is tricky on multi-homed hosts. BOSCO requires the submit host to be able to connect back to the BOSCO host, so it must advertise an interface that is reachable from all the chosen submit hosts. E.g. a host with a NIC on a private network and one with a public IP address must advertise the public address if the submit hosts are outside of the private network. \nIn order to do that you have to:\n   * make sure that the name returned by the command =/bin/hostname -f= is the name resolving in the public address (e.g. =host  hostname -f = should return the public address). If not you should change it.\n   * edit =~/bosco/local.%RED%$HOST%ENDCOLOR%/condor_config.local= (HOST is the short host name) and add a line like =NETWORK_INTERFACE = xxx.xxx.xxx.xxx= , substituting xxx.xxx.xxx.xxx with the public IP address. This will tell BOSCO to use that address.  ---## Modifying maximum number of submitted jobs to a resource  Many clusters limit the number of jobs that can be submitted to the scheduler.  For PBS, we are able to detect this limit.  For SGE and LSF, we are not able to detect this limit.  In the cases where we cannot find the limit, we set the maximum number of jobs very conservatively, to a maximum of 10.  This includes both the number of idle and running jobs to the cluster.  The limit is specified in the condor config file =~/bosco/local.bosco/condor_config.local=, at the bottom.  Edit the value of the configuration variable =GRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE=  \nGRIDMANAGER_MAX_SUBMITTED_JOBS_PER_RESOURCE = %RED%10%ENDCOLOR%  ---## Custom submit properties\nBosco has the ability to add custom submit properties to every job submitted to a cluster.  On the cluster's login node (the BOSCO resource, the host you used at the end of the line when typing the =bosco_cluster --add= command), create the file", 
            "title": "Final steps"
        }, 
        {
            "location": "/BoscoInstall/#customscriptlocations", 
            "text": "PBS/SLURM  - =~/bosco/glite/bin/pbs_local_submit_attributes.sh=  Condor  - =~/bosco/glite/bin/condor_local_submit_attributes.sh=  SGE  (and other GE) - =~/bosco/glite/bin/sge_local_submit_attributes.sh=  LSF  - =~/bosco/glite/bin/lsf_local_submit_attributes.sh=   %IMPORTANT% This file is executed and the output is inserted into the submit script. I.e. It is not cat, use echo/cat statements in the script.  Below is an example =pbs_local_submit_attributes.sh= script which will cause every job submitted to this cluster through Bosco to request 1 node with 8 cores:", 
            "title": "CustomScriptLocations"
        }, 
        {
            "location": "/BoscoInstall/#binsh", 
            "text": "echo \"#PBS -l nodes=1:ppn=8\"  ---### Passing parameters to the custom submit properties.\nYou may also pass parameters to the custom scripts by adding a special parameter to the Bosco submit script.  For example, in your Bosco submit script, add:  \n...\n%RED%+remote_cerequirements = NumJobs == 100%ENDCOLOR%\n...\nqueue  After you submit this job to Bosco, it will execute the [[#CustomScriptLocations][custom scripts]] with, in this example, =NumJobs= set in the environment equal to =100=.  The custom script can take advantage of these values.  For example, a PBS script can use the !NumJobs:", 
            "title": "!/bin/sh"
        }, 
        {
            "location": "/BoscoInstall/#binsh_1", 
            "text": "echo \"#PBS -l select=$NumJobs\"  This will set the number of requested cores from PBS to !NumJobs specified in the original Bosco Submit file.  ---## Flocking to a BOSCO installation\nIn some special cases you may desire to flock to your BOSCO installation. If you don't know what I'm talking about, then skip this section.  In order to enable flocking you must use an IP so that all the hosts you are flocking from can communicate with the BOSCO host.\nThen you must setup FLOCK_FROM and the security configuration so that the communications are authorized.  BOSCO has strong security settings. Here are two examples:\n   1 Using GSI authentication (a strong authentication method) you must provide and install X509 certificates, you must change the configuration %TWISTY{%TWISTY_OPTS_DETAILED% showlink=\"Click to see the configuration file\" }%    #", 
            "title": "!/bin/sh"
        }, 
        {
            "location": "/BoscoInstall/#networking-if-you-did-not-already-remember-that-you-need-to-set-bosco-not-to-use-the-loopback-port", 
            "text": "", 
            "title": "Networking - If you did not already, remember that you need to set BOSCO not to use the loopback port"
        }, 
        {
            "location": "/BoscoInstall/#hosts-definition", 
            "text": "", 
            "title": "Hosts definition"
        }, 
        {
            "location": "/BoscoInstall/#bosco-host", 
            "text": "H_BOSCO = %RED%bosco.mydomain.edu%ENDCOLOR%\nH_BOSCO_DN = %RED%/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=bosco.mydomain.edu%ENDCOLOR%", 
            "title": "BOSCO host"
        }, 
        {
            "location": "/BoscoInstall/#submit-host-flocking-to-bosco-host", 
            "text": "H_SUB = %RED%sub.mydomain.edu%ENDCOLOR%\nH_SUB_DN = %RED%/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=sub.mydomain.edu%ENDCOLOR%", 
            "title": "submit host (flocking to BOSCO host)"
        }, 
        {
            "location": "/BoscoInstall/#flocking-configuration", 
            "text": "", 
            "title": "Flocking configuration"
        }, 
        {
            "location": "/BoscoInstall/#security-definitions", 
            "text": "", 
            "title": "Security definitions"
        }, 
        {
            "location": "/BoscoInstall/#assuming-system-wide-installed-ca-certificates", 
            "text": "GSI_DAEMON_DIRECTORY = /etc/grid-security", 
            "title": "Assuming system-wide installed CA certificates"
        }, 
        {
            "location": "/BoscoInstall/#this-hosts-certificates", 
            "text": "GSI_DAEMON_CERT = /etc/grid-security/hostcert.pem\nGSI_DAEMON_KEY = /etc/grid-security/hostkey.pem", 
            "title": "This host's certificates"
        }, 
        {
            "location": "/BoscoInstall/#default-gsi_daemon_trusted_ca_dir-gsi_daemon_directorycertificates", 
            "text": "CERTIFICATE_MAPFILE= $HOME/bosco/local.bosco/certs/condor_mapfile", 
            "title": "default GSI_DAEMON_TRUSTED_CA_DIR = $(GSI_DAEMON_DIRECTORY)/certificates"
        }, 
        {
            "location": "/BoscoInstall/#not-used", 
            "text": "MY_DN = $(H_BOSCO_DN)", 
            "title": "Not used"
        }, 
        {
            "location": "/BoscoInstall/#who-to-trust-include-the-submitters-flocking-here", 
            "text": "GSI_DAEMON_NAME = $(GSI_DAEMON_NAME), $(H_BOSCO_DN), $(H_SUB_DN)", 
            "title": "Who to trust?  Include the submitters flocking here"
        }, 
        {
            "location": "/BoscoInstall/#enable-authentication-from-the-negotiator", 
            "text": "SEC_ENABLE_MATCH_PASSWORD_AUTHENTICATION = TRUE", 
            "title": "Enable authentication from the Negotiator"
        }, 
        {
            "location": "/BoscoInstall/#enable-gsi-authentication-and-claimtobe-for-campus-factories", 
            "text": "", 
            "title": "Enable gsi authentication, and claimtobe (for campus factories)"
        }, 
        {
            "location": "/BoscoInstall/#the-default-unix-should-be-fs-kerberos-gsi", 
            "text": "SEC_DEFAULT_AUTHENTICATION_METHODS = FS,GSI, PASSWORD, $(SEC_DEFAULT_AUTHENTICATION_METHODS)\nSEC_CLIENT_AUTHENTICATION_METHODS = FS, PASSWORD, GSI, CLAIMTOBE\nSEC_DAEMON_AUTHENTICATION_METHODS = FS, PASSWORD, GSI, CLAIMTOBE\nSEC_WRITE_AUTHENTICATION_METHODS = FS, PASSWORD, GSI, CLAIMTOBE\nSEC_ADVERTISE_SCHEDD_METHODS = FS, PASSWORD, GSI, CLAIMTOBE  ALLOW_DAEMON = $(ALLOW_DAEMON) condor_pool@ /  %RED%boscouser%ENDCOLOR%@ /  $(FULL_HOSTNAME) $(IP_ADDRESS)\nALLOW_ADVERTISE_SCHEDD = %RED%boscouser%ENDCOLOR%@ /   %ENDTWISTY% and define or update the condor_mapfile (e.g. =$HOME/bosco/local.bosco/certs/condor_mapfile=) %TWISTY{%TWISTY_OPTS_DETAILED% showlink=\"Click to see the condor_mapfile\" }%    #\nGSI \"^%RED%\\/DC\\=com\\/DC\\=DigiCert-Grid\\/O\\=Open\\ Science\\ Grid\\/OU\\=Services\\/CN\\=sub.mydomain.edu%ENDCOLOR%$\" %RED%boscouser@sub.mydomain.edu%ENDCOLOR%\nGSI \"^%RED%\\/DC\\=com\\/DC\\=DigiCert-Grid\\/O\\=Open\\ Science\\ Grid\\/OU\\=Services\\/CN\\=bosco.mydomain.edu%ENDCOLOR%$\" %RED%boscouser%ENDCOLOR%", 
            "title": "The default (unix) should be: FS, KERBEROS, GSI"
        }, 
        {
            "location": "/BoscoInstall/#networking-if-you-did-not-already-remember-that-you-need-to-set-bosco-not-to-use-the-loopback-port_1", 
            "text": "", 
            "title": "Networking - If you did not already, remember that you need to set BOSCO not to use the loopback port"
        }, 
        {
            "location": "/BoscoInstall/#flocking-configuration_1", 
            "text": "", 
            "title": "Flocking configuration"
        }, 
        {
            "location": "/BoscoInstall/#security-definitions-overrides", 
            "text": "", 
            "title": "Security definitions overrides"
        }, 
        {
            "location": "/BoscoInstall/#to-allow-status-read", 
            "text": "SEC_READ_INTEGRITY = OPTIONAL  SEC_CLIENT_AUTHENTICATION_METHODS = FS, PASSWORD, CLAIMTOBE  ALLOW_ADVERTISE_SCHEDD = */%RED%IP_of_the_host_in_flock_from%ENDCOLOR% $(FULL_HOSTNAME) $(IP_ADDRESS) $(ALLOW_DAEMON)  SEC_DAEMON_AUTHENTICATION = PREFERRED\nSEC_DAEMON_INTEGRITY = PREFERRED\nSEC_DAEMON_AUTHENTICATION_METHODS = FS,PASSWORD,CLAIMTOBE\nSEC_WRITE_AUTHENTICATION_METHODS = FS,PASSWORD,CLAIMTOBE  %ENDTWISTY%  After copying from the examples (click above to expand the example files) or editing your configuration file, save it as =$HOME/bosco/local.bosco/config/zzz_condor_config.flocking=. \nOther names are OK as long as its definition override the default ones of BOSCO (check with =condor_config_val -config=).  Then stop and restart BOSCO.  %ENDSECTION{\"BoscoAdvancedUseInstContent\"}%\n%ENDSECTION{\"BoscoAdvancedUse\"}%  ---# Troubleshooting  ---## Useful Configuration and Log Files\nBOSCO underneath is using Condor. You can find all the Condor log files in =~/bosco/local.HOSTNAME/log=  %STARTSECTION{\"BoscoTroubleshootingItems\"}%\n---## Make sure that you can connect to the BOSCO host\nIf you see errors like: \n Installing BOSCO on user@osg-ss-submit.chtc.wisc.edu...\n ssh: connect to host osg-ss-submit.chtc.wisc.edu port 22: Connection timed out\n rsync: connection unexpectedly closed (0 bytes received so far) [sender]\n rsync error: unexplained error (code 255) at io.c(600) [sender=3.0.6]\n ssh: connect to host osg-ss-submit.chtc.wisc.edu port 22: Connection timed out\n rsync: connection unexpectedly closed (0 bytes received so far) [sender]\n rsync error: unexplained error (code 255) at io.c(600) [sender=3.0.6]\n ssh: connect to host osg-ss-submit.chtc.wisc.edu port 22: Connection timed out\n rsync: connection unexpectedly closed (0 bytes received so far) [sender]\n rsync error: unexplained error (code 255) at io.c(600) [sender=3.0.6] \nPlease try manually to ssh from the BOSCO host to the cluster submit node. The ability to connect is required in order to install BOSCO.  ---## Make sure that BOSCO is running\nBOSCO may not survive after you log out, for example if the BOSCO node was restarted while you where logged out. \nWhen you log back in after sourcing the setup as described in the [[#SetupEnvironment][setup environment section]], you should start BOSCO as described in the [[#BoscoStart][BOSCO start section]], specially if the command =condor_q= is failing.  ---## Errors due to leftover files\nBosco files on the submit host are in:\n   * =~/bosco/= - the release directory\n   * =~/.bosco/= - some service files\n   * =~/.ssh/= - the ssh key used by BOSCO  If you used =bosco_uninstall= it will remove all BOSCO related files. If you removed BOSCO by hand you must pay attention.\nIf the service key is still in =.ssh= but the other files are missing, during the execution of BOSCO commands you will get some unclear errors like  \"IOError: [Errno 2] No such file or directory: '/home/marco/.bosco/.pass'\"  ,  \"OSError: [Errno 5] Input/output error\"  , all followed by: Password-less ssh to marco@itb2.uchicago.edu did NOT work, even after adding the ssh-keys.\nDoes the remote resource allow password-less ssh?  If that happens you can remove the service files and the keys using: rm -r ~/.bosco\nrm ~/.ssh/bosco_key.rsa* \nand then re-add all the clusters with =bosco_cluster --add=.  ---## Unable to download and prepare BOSCO for remote installation. \nBOSCO can return this error:\n   1. Because the BOSCO submit host is unable to download BOSCO for the resource installation, e.g. a firewall is blocking the download or the server is down\n   2. More commonly because there are problems with the login host of the BOSCO resource, e.g. the disk is full or there are multiple login nodes\nYou can check 1 byy downloading BOSCO on your BOSCO submit host.\nTo check 2 you have to login on the BOSCO resource: =df= will tell you you some disks are full, with =hostname -f= you can check if the name is different form the one that you used to login with ssh. If the name differs probably you are using a cluster with multiple login nodes and you must use only one for BOSCO. Se the second \"IMPORTANT\" note in the [[#AddResourceSection][section to add a cluster to BOSCO]] (above).  If you see errors similar to the one below while executing ==bosco_cluster --add==: \nDownloading for USER@RESOURCE\nUnpacking.tar: Cannot save working directory \ntar: Error is not recoverable: exiting now \nls: /tmp/tmp.qeIJ9139/condor*: No such file or directory \nUnable to download and prepare BOSCO for remote installation.  \nthen you are using most likely the generic name of a multi-login cluster and you should use the name of one of the nodes as suggested in the [[#AddResourceSection][note above]].   %ENDSECTION{\"BoscoTroubleshootingItems\"}%  ---# Get Help/Support\nTo get assistance you can send an email to bosco-discuss@opensciencegrid.org  %STARTSECTION{\"BoscoReferences\"}%\n---# References \n[[http://bosco.opensciencegrid.org/][BoSCO Web site]] and documents about the latest production release (v1.2)\n   * [[BoSCO][Using Bosco]]\n   * [[BoscoInstall][Installing BoSCO]]\n   * [[BoscoMultiUser][Installing BoSCO Multi User]]\n   * [[BoscoQuickStart][Quick start guide to Bosco]]\n   * BoscoR  Campus Grids related documents:\n   * https://twiki.grid.iu.edu/bin/view/CampusGrids\n   * https://twiki.grid.iu.edu/bin/view/Documentation/CampusFactoryInstall  Condor documents:\n   * Condor manual: http://research.cs.wisc.edu/condor/manual/  How to submit Condor jobs:\n   * Tutorial: http://research.cs.wisc.edu/condor/tutorials/alliance98/submit/submit.html\n   * Condor manual: http://research.cs.wisc.edu/condor/manual/v7.6/2_5Submitting_Job.html  Developers documents:\n   * [[TestBoSCO][BOSCO tests for developers and testers]]\n   * [[BoscoRoadmap][BOSCO Roadmap (planned and desired features)]]  Here you can check out older releases:\n   * [[BoSCOv0][BOSCO version 0]]\n   * [[BoSCOv1][BOSCO version 1]]\n   * [[BoSCOv1p1][BOSCO version 1.1]]\n   * [[BoSCOv1p2][BOSCO version 1.2]]   CICiForum130418", 
            "title": "To allow status read"
        }
    ]
}